{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ref4al/BWeek1/blob/main/WeeklyProject_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b7ab8e",
      "metadata": {
        "id": "20b7ab8e"
      },
      "source": [
        "# Weekly Project: Image Classification with Transfer Learning\n",
        "\n",
        "In this project, you will build a complete image classification pipeline using transfer learning. You'll work with the dataset provided by your instructor.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Load and prepare image datasets for deep learning\n",
        "- Use pre-trained models for transfer learning\n",
        "- Implement two transfer learning strategies: fine-tuning and feature extraction\n",
        "- Evaluate model performance\n",
        "- Deploy models using ONNX for production (Optional)\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Training with PyTorch](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html)\n",
        "- [PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70ba841",
      "metadata": {
        "id": "f70ba841"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Data Ingestion](#1)\n",
        "2. [Data Preparation](#2)\n",
        "3. [Model Building](#3)\n",
        "4. [Training](#4)\n",
        "   - [4.1 ConvNet as Fixed Feature Extractor](#4-1)\n",
        "   - [4.2 Fine-tuning the ConvNet](#4-2)\n",
        "5. [Evaluation](#5)\n",
        "6. [Inference on Custom Images](#6)\n",
        "7. [Deployment (ONNX)](#7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820b06f9",
      "metadata": {
        "id": "820b06f9"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms, models"
      ],
      "metadata": {
        "id": "6yyUrm9iTWuW"
      },
      "id": "6yyUrm9iTWuW",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a86c3ae9",
      "metadata": {
        "id": "a86c3ae9"
      },
      "source": [
        "## Setup Device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4057a953",
      "metadata": {
        "id": "4057a953"
      },
      "source": [
        "**Note: you will need a GPU; so please run this on Colab and specify a GPU runtime (e.g., T4-GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a16a3a31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16a3a31",
        "outputId": "e5f9ddf0-1fd6-43d1-829b-e41e66601b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlozUaOiYSdK",
        "outputId": "ae9d7193-92f3-4cd3-a327-cd923fd40dd2"
      },
      "id": "vlozUaOiYSdK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/B5-main/B5-main/intel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO0m1a4sI-ub",
        "outputId": "d8e1c563-e885-4b29-e3ec-5372a5d75fcb"
      },
      "id": "kO0m1a4sI-ub",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'archive (4).zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\"\n",
        "!unzip -q \"/content/drive/MyDrive/B5-main/B5-main/intel/archive (4).zip\" -d \"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\""
      ],
      "metadata": {
        "id": "NVU6jRZ8JIt4"
      },
      "id": "NVU6jRZ8JIt4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\" -maxdepth 4 -type d -name \"seg_train\" | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUHDqp-1LPlD",
        "outputId": "79013cd7-369f-4815-98dd-0958468bc05c"
      },
      "id": "xUHDqp-1LPlD",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_train\n",
            "/content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_train/seg_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7076ee",
      "metadata": {
        "id": "0d7076ee"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1. Data Ingestion\n",
        "\n",
        "**Task**: The dataset should be downloaded and extracted to a local directory.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Dataset and DataLoader](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html#dataset-and-dataloader)\n",
        "- [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "333a5ca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333a5ca9",
        "outputId": "fdb5f142-6ef6-419e-ec34-c255f0c28ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dir: /content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_train/seg_train\n",
            "test_dir : /content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_test/seg_test\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\")\n",
        "\n",
        "train_dir = DATA_ROOT / \"seg_train\" / \"seg_train\"\n",
        "test_dir  = DATA_ROOT / \"seg_test\"  / \"seg_test\"\n",
        "\n",
        "print(\"train_dir:\", train_dir)\n",
        "print(\"test_dir :\", test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2595f3e7",
      "metadata": {
        "id": "2595f3e7"
      },
      "source": [
        "**Task**: create a `train_dataset` and `test_dataset` (without transforms for now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5e31cf09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e31cf09",
        "outputId": "bc4af710-e0ee-43f4-896c-a11433c2890e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
            "Train size: 14034\n",
            "Test size : 3000\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_dataset = ImageFolder(str(train_dir))\n",
        "test_dataset  = ImageFolder(str(test_dir))\n",
        "\n",
        "print(\"Classes:\", train_dataset.classes)\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Test size :\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b555b9a0",
      "metadata": {
        "id": "b555b9a0"
      },
      "source": [
        "**Quick Check**: verify the counts of both train and test sets, match what's in the original source (Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9b85fba1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b85fba1",
        "outputId": "037254dc-1e33-4096-e66a-a444969540b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train counts per class:\n",
            "buildings : 2191\n",
            "forest    : 2271\n",
            "glacier   : 2404\n",
            "mountain  : 2512\n",
            "sea       : 2274\n",
            "street    : 2382\n",
            "\n",
            "Test counts per class:\n",
            "buildings : 437\n",
            "forest    : 474\n",
            "glacier   : 553\n",
            "mountain  : 525\n",
            "sea       : 510\n",
            "street    : 501\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from collections import Counter\n",
        "\n",
        "train_counts = Counter(train_dataset.targets)\n",
        "test_counts  = Counter(test_dataset.targets)\n",
        "\n",
        "print(\"Train counts per class:\")\n",
        "for i, name in enumerate(train_dataset.classes):\n",
        "    print(f\"{name:10s}: {train_counts[i]}\")\n",
        "\n",
        "print(\"\\nTest counts per class:\")\n",
        "for i, name in enumerate(test_dataset.classes):\n",
        "    print(f\"{name:10s}: {test_counts[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba2e137",
      "metadata": {
        "id": "2ba2e137"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. Data Preparation\n",
        "\n",
        "Before training, we need to:\n",
        "1. Define augmentation for training\n",
        "2. Define normalization for both training and testing\n",
        "3. Create **`DataLoader`** for efficient batch processing\n",
        "\n",
        "**Task:** Create transformation pipelines for training and validation. Pre-trained models expect ImageNet normalization statistics.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a4f5a9fd",
      "metadata": {
        "id": "a4f5a9fd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision import transforms\n",
        "img_size = 224\n",
        "\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size + 32),\n",
        "    transforms.RandomResizedCrop(img_size),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size + 32),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "51a000f4",
      "metadata": {
        "id": "51a000f4"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = ImageFolder(str(train_dir), transform=train_transform)\n",
        "test_dataset  = ImageFolder(str(test_dir),  transform=test_transform)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/B5-main/B5-main/WeeklyProject\"\n",
        "import helper_utils\n",
        "dir(helper_utils)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SttQimDoiv30",
        "outputId": "72d8727a-7d95-4f91-e138-b417f570b5af"
      },
      "id": "SttQimDoiv30",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B5-main/B5-main/WeeklyProject\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'imshow',\n",
              " 'np',\n",
              " 'plt',\n",
              " 'predict_single_image',\n",
              " 'torch',\n",
              " 'torchvision',\n",
              " 'visualize_batch',\n",
              " 'visualize_predictions',\n",
              " 'visualize_single_prediction',\n",
              " 'visualize_training_history']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2353e5",
      "metadata": {
        "id": "5f2353e5"
      },
      "source": [
        "**Quick Check**: Visualize a batch of training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3e042be0",
      "metadata": {
        "id": "3e042be0"
      },
      "outputs": [],
      "source": [
        "helper_utils.visualize_batch?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = ImageFolder(str(train_dir), transform=train_transform)\n",
        "test_dataset  = ImageFolder(str(test_dir),  transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(len(train_dataset), len(test_dataset))\n",
        "print(train_dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oFaJgiIgBiA",
        "outputId": "44e3cb43-d93e-42d9-8557-8cae39c99d46"
      },
      "id": "6oFaJgiIgBiA",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14034 3000\n",
            "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5402f6f8",
      "metadata": {
        "id": "5402f6f8"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Model Building\n",
        "\n",
        "We'll use a pre-trained ResNet-18 model and adapt it for our 6-class classification task.\n",
        "\n",
        "**Task:** Load a pre-trained ResNet-18 model and modify the final layer for 6 classes.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "- [torchvision.models](https://pytorch.org/vision/stable/models.html)\n",
        "- [ResNet documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "54179b13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54179b13",
        "outputId": "4952e8ab-f974-47f7-ec9f-542057721d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 67.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=512, out_features=6, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "print(model.fc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f6e9f8",
      "metadata": {
        "id": "d5f6e9f8"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Training\n",
        "\n",
        "**Task:** Implement a training **function** and then train using two different transfer learning strategies.\n",
        "\n",
        "**Reference:** [PyTorch Training Tutorial](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "be9f00c6",
      "metadata": {
        "id": "be9f00c6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import torch\n",
        "\n",
        "def train_model(model, train_loader, test_loader, loss_fn, opt, device, epochs=3):\n",
        "    for e in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        tr_loss, tr_ok, tr_n = 0.0, 0, 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            tr_loss += loss.item() * x.size(0)\n",
        "            tr_ok += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "        model.eval()\n",
        "        te_loss, te_ok, te_n = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                loss = loss_fn(out, y)\n",
        "\n",
        "                te_loss += loss.item() * x.size(0)\n",
        "                te_ok += (out.argmax(1) == y).sum().item()\n",
        "                te_n += y.size(0)\n",
        "\n",
        "        print(f\"Epoch {e+1}/{epochs}  \"\n",
        "              f\"train_acc={tr_ok/tr_n:.3f}  test_acc={te_ok/te_n:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train_model(model, train_loader, test_loader, loss_fn, opt, device, epochs=3)"
      ],
      "metadata": {
        "id": "4K-ZQc-QlcBd"
      },
      "id": "4K-ZQc-QlcBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "167c5f55",
      "metadata": {
        "id": "167c5f55"
      },
      "source": [
        "<a name='4-1'></a>\n",
        "### 4.1 ConvNet as Fixed Feature Extractor\n",
        "\n",
        "In this approach, we freeze all the convolutional layers and only train the final classifier layer.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Load a fresh pre-trained model\n",
        "2. Freeze all parameters except the final layer\n",
        "3. Set up optimizer to only train the final layer\n",
        "4. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1991e5f4",
      "metadata": {
        "id": "1991e5f4"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df38ca2f",
      "metadata": {
        "id": "df38ca2f"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f07a92a",
      "metadata": {
        "id": "6f07a92a"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a59b732",
      "metadata": {
        "id": "8a59b732"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48895088",
      "metadata": {
        "id": "48895088"
      },
      "source": [
        "**Quick Check**: Visualize training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e574c0",
      "metadata": {
        "id": "41e574c0"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_training_history(history_conv)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7323ab",
      "metadata": {
        "id": "9b7323ab"
      },
      "source": [
        "**Quick Check**: Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd724dd5",
      "metadata": {
        "id": "cd724dd5"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_predictions(model_conv, dataloaders['val'], class_names, device, num_images=6)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35323cc",
      "metadata": {
        "id": "f35323cc"
      },
      "source": [
        "<a name='4-2'></a>\n",
        "### 4.2 Fine-tuning the ConvNet\n",
        "\n",
        "In this approach, we unfreeze all layers and train the entire network with a smaller learning rate.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Load a fresh pre-trained model\n",
        "2. Modify the final layer\n",
        "3. Set up optimizer for all parameters with a smaller learning rate\n",
        "4. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cc8a65",
      "metadata": {
        "id": "67cc8a65"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3390ecfd",
      "metadata": {
        "id": "3390ecfd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8aa7ec",
      "metadata": {
        "id": "da8aa7ec"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75faec80",
      "metadata": {
        "id": "75faec80"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aca1a79",
      "metadata": {
        "id": "3aca1a79"
      },
      "source": [
        "**Quick Check**: Visualize training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635bcbd2",
      "metadata": {
        "id": "635bcbd2"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_training_history(history_ft)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7410d1d",
      "metadata": {
        "id": "e7410d1d"
      },
      "source": [
        "**Quick Check**: Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ac77fd",
      "metadata": {
        "id": "69ac77fd"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_predictions(model_ft, dataloaders['val'], class_names, device, num_images=6)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c455a085",
      "metadata": {
        "id": "c455a085"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Evaluation\n",
        "\n",
        "Compare the performance of both approaches.\n",
        "\n",
        "**Task:** Evaluate both models and compare their performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a198cc",
      "metadata": {
        "id": "28a198cc"
      },
      "outputs": [],
      "source": [
        "# Evaluate models on validation set\n",
        "# YOUR CODE HERE\n",
        "# Compare final validation accuracies, training times, etc.\n",
        "\n",
        "# Print comparison\n",
        "# print(\"Feature Extractor Approach:\")\n",
        "# print(f\"  Best Val Accuracy: {max(history_conv['val_acc']):.4f}\")\n",
        "# print(f\"  Final Val Accuracy: {history_conv['val_acc'][-1]:.4f}\")\n",
        "# print()\n",
        "# print(\"Fine-tuning Approach:\")\n",
        "# print(f\"  Best Val Accuracy: {max(history_ft['val_acc']):.4f}\")\n",
        "# print(f\"  Final Val Accuracy: {history_ft['val_acc'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44d1764",
      "metadata": {
        "id": "d44d1764"
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6. Inference on Custom Images\n",
        "\n",
        "Test your trained model on custom images.\n",
        "\n",
        "**Task:** Load a custom image, preprocess it, and make a prediction using your trained model.\n",
        "\n",
        "**Reference:** [Image Preprocessing](https://pytorch.org/vision/stable/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf5b0f3",
      "metadata": {
        "id": "ddf5b0f3"
      },
      "outputs": [],
      "source": [
        "# Make prediction on a custom image\n",
        "# img_path = 'path/to/your/image.jpg'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Use helper_utils.visualize_single_prediction or helper_utils.predict_single_image\n",
        "# helper_utils.visualize_single_prediction(\n",
        "#     model_ft,  # or model_conv\n",
        "#     img_path,\n",
        "#     data_transforms['val'],\n",
        "#     class_names,\n",
        "#     device\n",
        "# )\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb4ced4",
      "metadata": {
        "id": "1cb4ced4"
      },
      "source": [
        "# üèÜüéâ Congratulations on completing the Weekly Final Project! üéâüèÜ\n",
        "\n",
        "Fantastic job on finishing the Weekly Final Project! You‚Äôve put your skills to the test and made it to the end. Take a moment to celebrate your hard work and dedication. Keep up the great work and continue your learning journey!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0919579",
      "metadata": {
        "id": "e0919579"
      },
      "source": [
        "<a name='7'></a>\n",
        "## 7. Deployment (ONNX)\n",
        "\n",
        "Convert your trained model to ONNX format for deployment.\n",
        "\n",
        "**Task:**\n",
        "1. Convert the PyTorch model to ONNX format\n",
        "2. Load the ONNX model and perform inference\n",
        "\n",
        "**Reference:**\n",
        "- [PyTorch to ONNX](https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2573ceee",
      "metadata": {
        "id": "2573ceee"
      },
      "outputs": [],
      "source": [
        "# Convert model to ONNX\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Set model to evaluation mode\n",
        "# model_ft.eval()\n",
        "\n",
        "# Create dummy input (batch_size=1, channels=3, height=224, width=224)\n",
        "# dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "# Export to ONNX\n",
        "# onnx_path = 'model.onnx'\n",
        "# torch.onnx.export(\n",
        "#     model_ft,\n",
        "#     dummy_input,\n",
        "#     onnx_path,\n",
        "#     input_names=['input'],\n",
        "#     output_names=['output'],\n",
        "#     dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "# )\n",
        "\n",
        "# print(f\"Model exported to {onnx_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed08bba",
      "metadata": {
        "id": "fed08bba"
      },
      "outputs": [],
      "source": [
        "# Load ONNX model and perform inference\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Load ONNX model\n",
        "# ort_session = ort.InferenceSession(onnx_path)\n",
        "\n",
        "# Prepare input (use validation transform)\n",
        "# img_path = 'path/to/test/image.jpg'\n",
        "# img = Image.open(img_path).convert('RGB')\n",
        "# img_tensor = data_transforms['val'](img).unsqueeze(0)\n",
        "# img_numpy = img_tensor.numpy()\n",
        "\n",
        "# Run inference\n",
        "# outputs = ort_session.run(None, {'input': img_numpy})\n",
        "# predictions = np.array(outputs[0])\n",
        "# pred_class_idx = np.argmax(predictions[0])\n",
        "# pred_class = class_names[pred_class_idx]\n",
        "# confidence = np.max(predictions[0])\n",
        "\n",
        "# print(f\"Predicted: {pred_class} (confidence: {confidence:.2%})\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}