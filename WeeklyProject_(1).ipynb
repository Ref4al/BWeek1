{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ref4al/BWeek1/blob/main/WeeklyProject_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b7ab8e",
      "metadata": {
        "id": "20b7ab8e"
      },
      "source": [
        "# Weekly Project: Image Classification with Transfer Learning\n",
        "\n",
        "In this project, you will build a complete image classification pipeline using transfer learning. You'll work with the dataset provided by your instructor.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Load and prepare image datasets for deep learning\n",
        "- Use pre-trained models for transfer learning\n",
        "- Implement two transfer learning strategies: fine-tuning and feature extraction\n",
        "- Evaluate model performance\n",
        "- Deploy models using ONNX for production (Optional)\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Training with PyTorch](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html)\n",
        "- [PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70ba841",
      "metadata": {
        "id": "f70ba841"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Data Ingestion](#1)\n",
        "2. [Data Preparation](#2)\n",
        "3. [Model Building](#3)\n",
        "4. [Training](#4)\n",
        "   - [4.1 ConvNet as Fixed Feature Extractor](#4-1)\n",
        "   - [4.2 Fine-tuning the ConvNet](#4-2)\n",
        "5. [Evaluation](#5)\n",
        "6. [Inference on Custom Images](#6)\n",
        "7. [Deployment (ONNX)](#7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820b06f9",
      "metadata": {
        "id": "820b06f9"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms, models"
      ],
      "metadata": {
        "id": "6yyUrm9iTWuW"
      },
      "id": "6yyUrm9iTWuW",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a86c3ae9",
      "metadata": {
        "id": "a86c3ae9"
      },
      "source": [
        "## Setup Device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4057a953",
      "metadata": {
        "id": "4057a953"
      },
      "source": [
        "**Note: you will need a GPU; so please run this on Colab and specify a GPU runtime (e.g., T4-GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a16a3a31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16a3a31",
        "outputId": "9a7ca873-1275-4ce0-a648-b98fc9ee845b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlozUaOiYSdK",
        "outputId": "64b3f239-81ea-43d5-985e-bb279c9ba27a"
      },
      "id": "vlozUaOiYSdK",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/B5-main/B5-main/intel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO0m1a4sI-ub",
        "outputId": "e4c3926c-8960-434d-fe6f-dd20a4267f59"
      },
      "id": "kO0m1a4sI-ub",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'archive (4).zip'   extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\"\n",
        "!unzip -q \"/content/drive/MyDrive/B5-main/B5-main/intel/archive (4).zip\" -d \"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVU6jRZ8JIt4",
        "outputId": "7526de09-9bb2-49db-c0a5-48b58c2f137e"
      },
      "id": "NVU6jRZ8JIt4",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_pred/seg_pred/10004.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\" -maxdepth 4 -type d -name \"seg_train\" | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUHDqp-1LPlD",
        "outputId": "9bec4f5c-6dae-45e2-843e-d6c0592590e7"
      },
      "id": "xUHDqp-1LPlD",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_train\n",
            "/content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_train/seg_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7076ee",
      "metadata": {
        "id": "0d7076ee"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1. Data Ingestion\n",
        "\n",
        "**Task**: The dataset should be downloaded and extracted to a local directory.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Dataset and DataLoader](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html#dataset-and-dataloader)\n",
        "- [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "333a5ca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333a5ca9",
        "outputId": "6662535c-b686-4d64-ae3e-7e2e63c0534f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dir: /content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_train/seg_train\n",
            "test_dir : /content/drive/MyDrive/B5-main/B5-main/intel/extracted/seg_test/seg_test\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/B5-main/B5-main/intel/extracted\")\n",
        "\n",
        "train_dir = DATA_ROOT / \"seg_train\" / \"seg_train\"\n",
        "test_dir  = DATA_ROOT / \"seg_test\"  / \"seg_test\"\n",
        "\n",
        "print(\"train_dir:\", train_dir)\n",
        "print(\"test_dir :\", test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2595f3e7",
      "metadata": {
        "id": "2595f3e7"
      },
      "source": [
        "**Task**: create a `train_dataset` and `test_dataset` (without transforms for now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5e31cf09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e31cf09",
        "outputId": "f78ed5d6-b1d8-4ef0-d7f3-971323c152b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
            "Train size: 14034\n",
            "Test size : 3000\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_dataset = ImageFolder(str(train_dir))\n",
        "test_dataset  = ImageFolder(str(test_dir))\n",
        "\n",
        "print(\"Classes:\", train_dataset.classes)\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Test size :\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b555b9a0",
      "metadata": {
        "id": "b555b9a0"
      },
      "source": [
        "**Quick Check**: verify the counts of both train and test sets, match what's in the original source (Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9b85fba1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b85fba1",
        "outputId": "bd1f9b4d-9fc4-4f86-fe9a-0f03cf6fa1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:\n",
            "buildings 2191\n",
            "forest 2271\n",
            "glacier 2404\n",
            "mountain 2512\n",
            "sea 2274\n",
            "street 2382\n",
            "\n",
            "Test:\n",
            "buildings 437\n",
            "forest 474\n",
            "glacier 553\n",
            "mountain 525\n",
            "sea 510\n",
            "street 501\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from collections import Counter\n",
        "\n",
        "train_counts = Counter(train_dataset.targets)\n",
        "test_counts  = Counter(test_dataset.targets)\n",
        "\n",
        "print(\"Train:\")\n",
        "for i, name in enumerate(train_dataset.classes):\n",
        "    print(name, train_counts[i])\n",
        "\n",
        "print(\"\\nTest:\")\n",
        "for i, name in enumerate(test_dataset.classes):\n",
        "    print(name, test_counts[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba2e137",
      "metadata": {
        "id": "2ba2e137"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. Data Preparation\n",
        "\n",
        "Before training, we need to:\n",
        "1. Define augmentation for training\n",
        "2. Define normalization for both training and testing\n",
        "3. Create **`DataLoader`** for efficient batch processing\n",
        "\n",
        "**Task:** Create transformation pipelines for training and validation. Pre-trained models expect ImageNet normalization statistics.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a4f5a9fd",
      "metadata": {
        "id": "a4f5a9fd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision import transforms\n",
        "img_size = 224\n",
        "\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size + 32),\n",
        "    transforms.RandomResizedCrop(img_size),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size + 32),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "51a000f4",
      "metadata": {
        "id": "51a000f4"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = ImageFolder(str(train_dir), transform=train_transform)\n",
        "test_dataset  = ImageFolder(str(test_dir),  transform=test_transform)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/B5-main/B5-main/WeeklyProject\"\n",
        "import helper_utils\n",
        "dir(helper_utils)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SttQimDoiv30",
        "outputId": "485fcb54-2785-40df-e539-b62ccf83563d"
      },
      "id": "SttQimDoiv30",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B5-main/B5-main/WeeklyProject\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'imshow',\n",
              " 'np',\n",
              " 'plt',\n",
              " 'predict_single_image',\n",
              " 'torch',\n",
              " 'torchvision',\n",
              " 'visualize_batch',\n",
              " 'visualize_predictions',\n",
              " 'visualize_single_prediction',\n",
              " 'visualize_training_history']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2353e5",
      "metadata": {
        "id": "5f2353e5"
      },
      "source": [
        "**Quick Check**: Visualize a batch of training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3e042be0",
      "metadata": {
        "id": "3e042be0"
      },
      "outputs": [],
      "source": [
        "helper_utils.visualize_batch?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = ImageFolder(str(train_dir), transform=train_transform)\n",
        "test_dataset  = ImageFolder(str(test_dir),  transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(len(train_dataset), len(test_dataset))\n",
        "print(train_dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oFaJgiIgBiA",
        "outputId": "48c9e17b-3d18-4a03-be1d-aca0c1be47da"
      },
      "id": "6oFaJgiIgBiA",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14034 3000\n",
            "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5402f6f8",
      "metadata": {
        "id": "5402f6f8"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Model Building\n",
        "\n",
        "We'll use a pre-trained ResNet-18 model and adapt it for our 6-class classification task.\n",
        "\n",
        "**Task:** Load a pre-trained ResNet-18 model and modify the final layer for 6 classes.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "- [torchvision.models](https://pytorch.org/vision/stable/models.html)\n",
        "- [ResNet documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "54179b13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54179b13",
        "outputId": "434505db-b5bf-4b24-9e18-f11e5a4f0f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=512, out_features=6, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "print(model.fc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f6e9f8",
      "metadata": {
        "id": "d5f6e9f8"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Training\n",
        "\n",
        "**Task:** Implement a training **function** and then train using two different transfer learning strategies.\n",
        "\n",
        "**Reference:** [PyTorch Training Tutorial](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "be9f00c6",
      "metadata": {
        "id": "be9f00c6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import torch\n",
        "\n",
        "def train_model(model, train_loader, test_loader, loss_fn, opt, device, epochs=1):\n",
        "    history = {\"train_acc\": [], \"test_acc\": []}\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        tr_ok, tr_n = 0, 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            tr_ok += (out.argmax(1) == y).sum().item()\n",
        "            tr_n  += y.size(0)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        te_ok, te_n = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "\n",
        "                te_ok += (out.argmax(1) == y).sum().item()\n",
        "                te_n  += y.size(0)\n",
        "\n",
        "        tr_acc = tr_ok / tr_n\n",
        "        te_acc = te_ok / te_n\n",
        "\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"test_acc\"].append(te_acc)\n",
        "\n",
        "        print(f\"Epoch {e+1}/{epochs}  train_acc={tr_acc:.3f}  test_acc={te_acc:.3f}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "167c5f55",
      "metadata": {
        "id": "167c5f55"
      },
      "source": [
        "<a name='4-1'></a>\n",
        "### 4.1 ConvNet as Fixed Feature Extractor\n",
        "\n",
        "In this approach, we freeze all the convolutional layers and only train the final classifier layer.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Load a fresh pre-trained model\n",
        "2. Freeze all parameters except the final layer\n",
        "3. Set up optimizer to only train the final layer\n",
        "4. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1991e5f4",
      "metadata": {
        "id": "1991e5f4"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "model_conv = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model_conv.fc = nn.Linear(model_conv.fc.in_features, num_classes)\n",
        "model_conv = model_conv.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "df38ca2f",
      "metadata": {
        "id": "df38ca2f"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "for p in model_conv.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for p in model_conv.fc.parameters():\n",
        "    p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6f07a92a",
      "metadata": {
        "id": "6f07a92a"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import torch\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model_conv.fc.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train batches:\", len(train_loader), \"test batches:\", len(test_loader))\n",
        "print(\"train samples:\", len(train_loader.dataset), \"test samples:\", len(test_loader.dataset))\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "print(\"one batch:\", x.shape, y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmM_LVFmXwPm",
        "outputId": "bcb2b030-2673-46d8-8ff9-2cddf027218e"
      },
      "id": "EmM_LVFmXwPm",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train batches: 439 test batches: 94\n",
            "train samples: 14034 test samples: 3000\n",
            "one batch: torch.Size([32, 3, 224, 224]) tensor([4, 3, 1, 5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8a59b732",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a59b732",
        "outputId": "226d619a-dda1-45de-eac7-f7d39d1bfb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1  train_acc=0.835  test_acc=0.890\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "history_conv = train_model(model_conv, train_loader, test_loader, loss_fn, opt, device, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48895088",
      "metadata": {
        "id": "48895088"
      },
      "source": [
        "**Quick Check**: Visualize training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "41e574c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "41e574c0",
        "outputId": "42086dd8-8590-45a4-e5f1-f9491092e010"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'train_loss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2890572208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhelper_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/B5-main/B5-main/WeeklyProject/helper_utils.py\u001b[0m in \u001b[0;36mvisualize_training_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Plot loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# helper_utils.visualize_training_history(history_conv)\n",
        "# plt.show()\n",
        "import helper_utils\n",
        "helper_utils.visualize_training_history(history_conv)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7323ab",
      "metadata": {
        "id": "9b7323ab"
      },
      "source": [
        "**Quick Check**: Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd724dd5",
      "metadata": {
        "id": "cd724dd5"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_predictions(model_conv, dataloaders['val'], class_names, device, num_images=6)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35323cc",
      "metadata": {
        "id": "f35323cc"
      },
      "source": [
        "<a name='4-2'></a>\n",
        "### 4.2 Fine-tuning the ConvNet\n",
        "\n",
        "In this approach, we unfreeze all layers and train the entire network with a smaller learning rate.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Load a fresh pre-trained model\n",
        "2. Modify the final layer\n",
        "3. Set up optimizer for all parameters with a smaller learning rate\n",
        "4. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cc8a65",
      "metadata": {
        "id": "67cc8a65"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3390ecfd",
      "metadata": {
        "id": "3390ecfd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8aa7ec",
      "metadata": {
        "id": "da8aa7ec"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75faec80",
      "metadata": {
        "id": "75faec80"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aca1a79",
      "metadata": {
        "id": "3aca1a79"
      },
      "source": [
        "**Quick Check**: Visualize training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635bcbd2",
      "metadata": {
        "id": "635bcbd2"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_training_history(history_ft)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7410d1d",
      "metadata": {
        "id": "e7410d1d"
      },
      "source": [
        "**Quick Check**: Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ac77fd",
      "metadata": {
        "id": "69ac77fd"
      },
      "outputs": [],
      "source": [
        "# helper_utils.visualize_predictions(model_ft, dataloaders['val'], class_names, device, num_images=6)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c455a085",
      "metadata": {
        "id": "c455a085"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Evaluation\n",
        "\n",
        "Compare the performance of both approaches.\n",
        "\n",
        "**Task:** Evaluate both models and compare their performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a198cc",
      "metadata": {
        "id": "28a198cc"
      },
      "outputs": [],
      "source": [
        "# Evaluate models on validation set\n",
        "# YOUR CODE HERE\n",
        "# Compare final validation accuracies, training times, etc.\n",
        "\n",
        "# Print comparison\n",
        "# print(\"Feature Extractor Approach:\")\n",
        "# print(f\"  Best Val Accuracy: {max(history_conv['val_acc']):.4f}\")\n",
        "# print(f\"  Final Val Accuracy: {history_conv['val_acc'][-1]:.4f}\")\n",
        "# print()\n",
        "# print(\"Fine-tuning Approach:\")\n",
        "# print(f\"  Best Val Accuracy: {max(history_ft['val_acc']):.4f}\")\n",
        "# print(f\"  Final Val Accuracy: {history_ft['val_acc'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44d1764",
      "metadata": {
        "id": "d44d1764"
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6. Inference on Custom Images\n",
        "\n",
        "Test your trained model on custom images.\n",
        "\n",
        "**Task:** Load a custom image, preprocess it, and make a prediction using your trained model.\n",
        "\n",
        "**Reference:** [Image Preprocessing](https://pytorch.org/vision/stable/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf5b0f3",
      "metadata": {
        "id": "ddf5b0f3"
      },
      "outputs": [],
      "source": [
        "# Make prediction on a custom image\n",
        "# img_path = 'path/to/your/image.jpg'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Use helper_utils.visualize_single_prediction or helper_utils.predict_single_image\n",
        "# helper_utils.visualize_single_prediction(\n",
        "#     model_ft,  # or model_conv\n",
        "#     img_path,\n",
        "#     data_transforms['val'],\n",
        "#     class_names,\n",
        "#     device\n",
        "# )\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb4ced4",
      "metadata": {
        "id": "1cb4ced4"
      },
      "source": [
        "# üèÜüéâ Congratulations on completing the Weekly Final Project! üéâüèÜ\n",
        "\n",
        "Fantastic job on finishing the Weekly Final Project! You‚Äôve put your skills to the test and made it to the end. Take a moment to celebrate your hard work and dedication. Keep up the great work and continue your learning journey!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0919579",
      "metadata": {
        "id": "e0919579"
      },
      "source": [
        "<a name='7'></a>\n",
        "## 7. Deployment (ONNX)\n",
        "\n",
        "Convert your trained model to ONNX format for deployment.\n",
        "\n",
        "**Task:**\n",
        "1. Convert the PyTorch model to ONNX format\n",
        "2. Load the ONNX model and perform inference\n",
        "\n",
        "**Reference:**\n",
        "- [PyTorch to ONNX](https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2573ceee",
      "metadata": {
        "id": "2573ceee"
      },
      "outputs": [],
      "source": [
        "# Convert model to ONNX\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Set model to evaluation mode\n",
        "# model_ft.eval()\n",
        "\n",
        "# Create dummy input (batch_size=1, channels=3, height=224, width=224)\n",
        "# dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "# Export to ONNX\n",
        "# onnx_path = 'model.onnx'\n",
        "# torch.onnx.export(\n",
        "#     model_ft,\n",
        "#     dummy_input,\n",
        "#     onnx_path,\n",
        "#     input_names=['input'],\n",
        "#     output_names=['output'],\n",
        "#     dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "# )\n",
        "\n",
        "# print(f\"Model exported to {onnx_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed08bba",
      "metadata": {
        "id": "fed08bba"
      },
      "outputs": [],
      "source": [
        "# Load ONNX model and perform inference\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Load ONNX model\n",
        "# ort_session = ort.InferenceSession(onnx_path)\n",
        "\n",
        "# Prepare input (use validation transform)\n",
        "# img_path = 'path/to/test/image.jpg'\n",
        "# img = Image.open(img_path).convert('RGB')\n",
        "# img_tensor = data_transforms['val'](img).unsqueeze(0)\n",
        "# img_numpy = img_tensor.numpy()\n",
        "\n",
        "# Run inference\n",
        "# outputs = ort_session.run(None, {'input': img_numpy})\n",
        "# predictions = np.array(outputs[0])\n",
        "# pred_class_idx = np.argmax(predictions[0])\n",
        "# pred_class = class_names[pred_class_idx]\n",
        "# confidence = np.max(predictions[0])\n",
        "\n",
        "# print(f\"Predicted: {pred_class} (confidence: {confidence:.2%})\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}